---
layout: post
title: 机器学习常用模型速写（五）支持向量机
date: 2020-06-24 12:25:06 -0700
tags: 
<!-- comments: true -->
categories: machine learning
---

支持向量机support vector machine简称为SVM方法。还是先举个例子，如下图所示的线性可分的二分类问题。方框和圆圈的数据点分别属于两个不同的类别，想要找到一个分类模型（二维情况就是一条直线，多维问题中我们称其为超平面（hyperplane），这里不作区分）将两个类别分开，可以作出无数条类似下图中的超平面。我们称这些分割的超平面为决策边界（decision boundary）。

![fig1]({{ site.baseurl}}/images/机器学习速写-支持向量机1.jpg)

图1：线性可分问题的可能的决策边界

哪一个模型是最优的呢？如下图所示的两个决策边界：B1和B2。两个边界虽然都能无误的区分两类，但是它们到最近的数据点的距离不同，按如下图中箭头所示的方向平行移动两个边界直到到达最邻近的一个数据点，这样B1,B2分别生成如图中虚线所示的两个超平面。这两条虚线超平面之间的距离叫做决策边界的边缘（margin）。

![fig2]({{ site.baseurl}}/images/机器学习速写-支持向量机2.jpg)

图2: 决策边界的边缘

我们选择或训练模型的目的是为了找到能更好的应用于未知分类数据而非已知数据的模型，而边缘越大，泛化能力越好。SVM的基本思想就是寻找具有最大边缘的决策边界 ，线性情况下称为最大边缘超平面 (maximal margin hyperplane)。边缘与泛化误差之间的关系在统计学中有相应的理论公式，叫做结构风险最小化 (structural risk minimization, SRM)理论。

线性SVM (linear SVM)就是寻找最大边缘超平面的模型。下图所示是一个思路图，决策边界的边缘d可以用含w的函数来表示d=2/||w||，这样我们要解决的问题概括起来就是函数求极值的问题，另外由于要保证方块点全部在边界上或上方，圆圈全部在边界下或下方，所以有个不等式的限制条件。这样就变成类似于一个线性规划问题。

![fig3]({{ site.baseurl}}/images/机器学习速写-支持向量机3.jpg)

图3: 寻找最大边缘超平面的思路图

实际中不直接求b的最大值，而是将其变形为求目标函数f(w)=||w||^2/2的极小值，变形后仍然和原问题等价。这时就变成求取具有线性约束的二次函数的极小值问题，属于称为凸优化 (convex)的一类问题。具体的公式如下所示，

![fig4]({{ site.baseurl}}/images/机器学习速写-支持向量机4.jpg)

求解的方法叫做标准拉格朗日乘子 (Lagrange multiplier)方法 （这个名字听起来有些熟悉，印象中理论力学中有应用，思路基本是很像的）。改写目标函数为如下的拉格朗日函数，

![fig5]({{ site.baseurl}}/images/机器学习速写-支持向量机5.jpg)

参数λi称为拉格朗日乘子。为了最小化这个拉格朗日函数，对w和b分别求偏导，另它们等于零。如下，

![fig6]({{ site.baseurl}}/images/机器学习速写-支持向量机6.jpg)

将求得的关系带入原拉格朗日函数，得到如下对偶形式 (dual formulation)

![fig7]({{ site.baseurl}}/images/机器学习速写-支持向量机7.jpg)

这个公式中就消掉了两个参数w和b，简化了函数。按书中的说法，接下来这个对偶优化问题可以用数值计算技术如二次规划来求解，求得λi后通过其与b,w的关系就可求解b和w。

在如下图所示的例子中，B2完全无误的分类了样本，B1错误分类了两个点P和Q，在这种情况下如果用前面所述的思路进行分类SVM就会选取B2作为决策边界，但是实际上P, Q很有可能是噪声点，选择B2有可能会过分拟合，这样就需要对之前的公式进行修正。

![fig8]({{ site.baseurl}}/images/机器学习速写-支持向量机8.jpg)

图4: 不可分情况下的决策边界

利用一种叫做软边缘 (soft margin)的方法，“软化”边缘，提高模型对错点的容忍度。这种方法通过在约束中加入松弛变量 (slack variable) ξ 来实现。如下所示，

![fig9]({{ site.baseurl}}/images/机器学习速写-支持向量机9.jpg)

为了对误分数据点在数量上有所限制，将目标函数修改为，

![fig10]({{ site.baseurl}}/images/机器学习速写-支持向量机10.jpg)

参数C为对误分点的惩罚项。然后按照如前所述同样的方法进行处理。

以上所述是线性SVM，构建的是线性的决策边界，如果要构造非线性的决策边界，例如解决如下图a中所示的问题，

![fig11]({{ site.baseurl}}/images/机器学习速写-支持向量机11.jpg)

图5: 分类具有非线性决策边界的数据

则需要通过坐标变换使得在新的坐标空间（用Φ表示）中，其是线性可分的（如上图b中所示）。在这个例子中，a图决策边界是个圆，分类公式为

![fig12]({{ site.baseurl}}/images/机器学习速写-支持向量机12.jpg)

通过如下形式变换到新的空间Φ。

![fig13]({{ site.baseurl}}/images/机器学习速写-支持向量机13.jpg)

新空间下的数据点变成了线性可分的，理论上可以用如前所述的同样的方法进行处理。但是之前的计算中涉及向量的乘积x_i*y_i（或称为相似度），而在新空间下直接计算向量的乘积Φ(x_i)*Φ(x_j)的计算量过大。这个问题通过核技术 (kernel trick)解决。其思路是空间变换要使得变换后空间向量的乘积Φ(x_i)*Φ(x_j) 等价于求取原空间向量的乘积x_i*y_i。对此有一个Mercer定理。这个把Φ(x_i)*Φ(x_j)表示成x_i*y_i函数的式子称为核函数 (kernel function)。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）