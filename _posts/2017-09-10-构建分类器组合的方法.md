---
layout: post
title: 构建分类器组合的方法
date: 2020-06-24 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

前面几节分别介绍了单个分类器模型，通过构建多个基分类器（base classifier），并将这些分类器组合在一起，可以提高分类的准确率，这种方法称为分类器组合（ensemble method）方法。

组合方法一般通过多数表决的方法确定一个数据点的最终分类，因此如果能保证这些基分类器之间相互独立，那么只有在多数的分类器都错误分类的时候，最终的结果才会出错。

构建多个基分类器的方法一般有下面几种：

（一）通过生成新的训练集.

基于某种抽样分布，对原始训练集中的数据进行抽样来构成多个新的训练集，然后为每个训练集建立一个分类器。根据不同的抽样方法一般有装袋（bagging）和提升 (boosting) 两种方法。

（二）利用部分（而不是全部）特征（属性）来构建分类器。

通过随机（或用某种更复杂的方法）选择一部分特征，即得到的每个分类器是只基于部分特征构建的。这种方法适用于特征比较多的数据集。随机森林（Random Forest）就是用这样的方法构建的，它的基分类器为决策树（Decision Tree）。

（三）对于分类类别较多的问题（例如：y_1, y_2...y_10），通过随机将这些类别分为两类（两组，如{y_1,y_5,y_9}, {y_2,y_3,y_4,y_6...}两组，可重新记为z_1, z_2），可以将训练集转换为二分类问题的训练集，在此基础上构建模型，然后重复重新分组和构建模型的过程，这样就得到多个分类器。最终的预测结果由各个分类器的结果投票决定。

（四）通过对模型算法进行处理。

比如对于人工神经网络可以通过改变它的结构或各结点的初始权值来构建多个分类器。也可以在模型中注入一些随机性，生成多个模型，比如对于决策树模型，可以随机的选择最优的k个特征中的一个，而不是固定选择一个。

下面介绍装袋（bagging）和提升（boosting）两种生成新训练集的方法。

装袋是一种完全随机的（基于均匀概率分布）从数据集中重复的有放回的抽样的方法。基于新的n个数据集构建n个模型，然后通过多数表决的方法，即票数最多的一类为测试点的最终分类。

不同于装袋只是简单的随机抽样，提升给每个样本赋一个权值，在每一轮结束时会自动调整权值，使得难分的样本出现的概率更高，以便分类器能更关注难分的样本。另外前一轮没有被选中的样本也有更大的概率在本轮被选中。除了对抽样过程进行权重调整，还可以对最终多数表决时的每个分类器的权重进行调整。AdaBoost就是基于这样的一种算法。其基于每个分类器的错误率为分类器的结果进行加权（错误率e和权重的关系如下图所示），降低错误率高的分类器在投票表决时的权重。另外AdaBoost算法对数据集本身也有类似上述的权重调整过程。由于其更关注难分类的样本点，也容易导致过分拟合。

![fig1]({{ site.baseurl}}/images/构建分类器组合的方法1.jpg)

图1：错误率e和权重的关系，错误率接近0时权重最大，错误率接近1 (100%) 时权重最小。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）