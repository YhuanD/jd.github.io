---
layout: post
title: 机器学习常用模型速写（六）K均值聚类
date: 2020-06-24 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

之前几节介绍的都是有监督学习模型，给定的训练集是已经带有类别标签的，建立模型的目的在于通过训练模型预测未知数据的标签，叫做分类 (classification) 问题；聚类 (clustering) 属于无监督学习模型，其是将不带标签的数据划分成有意义或有用的组（称为簇 (cluster)）。

聚类不仅在数据挖掘领域有应用，在其他领域也发挥很大的作用。比如在生物领域对所有生物体的系统有层次的分类：界 (kingdom)、门 (phylum)、纲 (class)、目 (order)、科 (family)、属 (genus)、种 (species)。应用方面，在信息领域，对数据的压缩也应用了聚类分析的相关方法。一般情况下，聚类分析只是解决其他问题的起点。其主要目标是使得组内对象之间是相似的（相关的），而不同组中的对象是不同的（不相关的），组内的相似性越大，组间差别越大，聚类越好。

聚类有几种不同的类型：层次的 (hierarchical clustering)与划分的 (partitional clustering)，互斥的、重叠的与模糊的，完全的与部分的。从字面上能理解这些类型的大概意思。层次的就是存在簇子集的情况，层层聚类，如图1中所示的例子，而划分的，就是只存在单层的情况。

![fig1]({{ site.baseurl}}/images/机器学习速写-k均值聚类1.jpg)

图1 层次聚类。 (a-d)分别具有1、2、4、6个簇

互斥的 (exclusive)，就是一个点不能属于多个簇，重叠的 (overlapping) 就是一个点可以属于多个簇，例如，一个人可以有多重身份，既是爸爸又是儿子。模糊聚类 (fuzzy clustering) 就是一个点属于（接近）多个簇但是对每个簇都有个概率，实际上模糊聚类不能真正解决一个点属于多个簇的问题，一般按照概率的大小将其转化为互斥聚类。完全聚类 (complete clustering) 是所有点都被归为某个簇，部分聚类 (partial clustering) 就是对于有些离群点、噪声点或不感兴趣的背景点，不将其归为任何簇。

以上一段讲了不同的聚类类型，簇也有不同的类型，包括：明显分离 (well-separated) 的，基于原型的 (prototype-based)，基于图的 (graph-based)，基于密度的 (density-based)，基于概念 (shared-property) 的。如下图所示的例子。

![fig2]({{ site.baseurl}}/images/机器学习速写-k均值聚类2.jpg)

图2 五种簇的类型举例

明显分离的簇（图2a）是不同的簇中任意两点的距离大于簇内两点的距离，其形状不一定为球形。原型指簇中最有代表性的点，对于连续属性的数据通常为质心，当质心没有意义时，例如数据具有分类属性的情况，原型通常为中心点。基于原型的簇（图2b）是指每个点到该簇内原型的距离比到其他簇原型的距离更近，这种簇通常成球状。基于图的簇（图2c）的一个重要例子是基于邻近的簇 (contiguity-based clusters)，其中每个点到该簇的某个点的距离比到不同簇中任意点的距离更近。基于密度的簇（图2c,d）指，簇为高密度区域，被低密度区域环绕，当簇形状不规则或相互盘绕，并且有噪声点和离群点时使用。基于共同性质的簇（图2e）或称概念簇，是指簇中的点都具有共同性质，这包含前面所有类型的簇，例如在基于中心的簇中，共同性质就是每个点离它们的质心或中心点最近，但是基于共同性质的簇还可以自定义其他的性质概念，从而产生新的簇类型。

以上介绍了聚类的一些基本概念和聚类类型以及簇类型。下面介绍其中的一种常用的基于原型的划分聚类方法-K均值聚类。

K均值算法的基本过程为：选择K个初始质心，K为用户指定的参数，即为想要聚成的簇的个数，然后将每个数据指派到‘距离’其最近的质心，这样就形成K个簇，然后再重新计算每个簇的质心，再重复指派的过程和更新质心的过程，直到质心不发生变化，这时所有点都不再从一个簇换到另一个簇。其中‘距离’的度量由邻近性度量（如欧几里得距离等）求得，而对邻近性度量和质心类型的一些组合，总能使以上过程收敛，大部分收敛都发生在早期阶段，所以一般只算到例如仅有1%的点不发生簇更换。

我们现在回到聚类最原始的目标，通常有个上一节提到过的目标函数，目标函数的其中一个例子就是我们比较熟悉的在最小二乘法中用到的误差平方和 (sum of the squared error, SSE)。我们聚类要达到的目标就是聚类结果要使得目标函数达到最大或最小值。例如，若将SSE作为目标函数，聚类最后的理想结果是使SSE最小，然而若使用上段所述的迭代的聚类过程则只能保证得到SSE局部最小值。例如在选择初始质心的阶段，如果每次纯粹随机的选取初始质心，则每次迭代都将收敛到不同的SSE值，并且常常会得到很差的聚类结果，对此书中有一些例子，说明只有当K=2也就是只设两个质心时才能避免低质量的聚类。因此，通常使用二分K均值方法来确定K均值聚类的初始质心，二分K均值是K均值方法的一个变种，基本过程是，先将所有点分成两个簇，再选择一个簇继续分裂，每次只将一个簇一分为二，直到得到K个簇。其中待分裂的簇有许多不同的选择方法，例如选择最大的簇，或选择SSE最大的簇，或者综合考虑簇的大小和簇SSE的大小。使用二分K均值方法分裂出K个簇之后，再将每个簇的质心作为K均值聚类的初始质心。

另一种降低SSE的方法是使用后处理方法，因为总的SSE实际上是每个簇的SSE的总和，后处理方法关注的就是单个簇的SSE，通过分裂有较大SSE的簇，或合并两个簇等方法对每一步得到的簇进行调整。另外离群点的存在对聚类的质量也将产生影响，可以使用一些方法，针对特定的问题对离群点在聚类之前或聚类过程中进行处理，提高聚类质量。

K均值能够用来处理等密度、等尺寸的球形簇，或者明显分离的簇，但是很难检测到不同尺度，或非球形，或不同密度的簇。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）

欢迎关注我的公众号获取更多内容

![fig]({{ site.baseurl}}/images/wechat_from0_data.jpg)