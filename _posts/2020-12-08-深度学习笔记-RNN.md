---
layout: post
title: 深度学习笔记-RNN
date: 2020-12-08 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

前一篇文章介绍了卷积神经网络CNN模型，CNN模型能够提取空间点间的关系特征，因此被广泛应用于图像识别领域。在实际应用中，我们会遇到一些依赖于时间的（或者说有序的）问题，比如一个人的一些列连续的动作，自然语言等等。自然语言是一个典型的有序问题，我们理解每个句子通常要依赖于前文所提供的信息，词语的顺序不同，整个句子的含义也会不同。同样，机器想要理解自然语言，就需要具备“记忆”功能。循环神经网络RNN就是一个“有记忆能力”的模型。

RNN模型是怎么具备这种记忆能力的呢？我想到一个可能不是很恰当的比喻，比如你想要熬一锅美味的汤，你先放了片羊肉，涮了半分钟，然后拿出来，这时候你可以吃到涮好的羊肉，同时汤里也有了羊肉的味道，这时候你又涮了点蔬菜，汤里又有了蔬菜和羊肉混合的味道（当然不是简单的物理混合，自行想象。。）后来你又涮了各种肉各种菜……汤的味道每次都不太一样，同样的食材涮出来的味道也和你涮的顺序有关。这就是一个有记忆的模型，其中有个关键点，就是你用的锅要是同一个。。。

RNN模型结构如下图所示，有两种展示方法，左图是折叠图，右图是展开图。从左图中可以看到对于每个时刻，模型都是同一个（共用参数VWU），这个结构与简单的前向神经网络的区别在于中间的那个循环结构，有了这个循环结构，t时刻的结果不但与t时刻的输入数据x_t有关，同时也与上一时刻的状态结果s_t-1有关，而实际上s_t-1又与上一时刻的状态s_t-2有关，由此推出t时刻的状态s_t和结果o_t实际上保留了所有之前的输入数据的信息。

![fig1]({{ site.baseurl}}/images/深度学习笔记-RNN1.png)

图1 RNN模型结构示意图。x为输入，o为输出，s为隐藏状态（隐藏层），UVW分别为各自的权重weights。

RNN模型的训练过程与普通前向神经网络一样，也是通过反向传播的方法更新权重矩阵weights。只是推导的过程更为复杂一些。这里就不说了，我也没仔细看，如果以后用到再看也不迟==，都是链式法则求偏导。

理论上讲，RNN模型能够保留无限长时间的信息，从而处理无限长的序列数据。然而实际上由于反向传播的计算结果中有近似W^t这个项存在，矩阵的t次方这个东西会随着时间t的增加而使W的特征值趋向两极化（小于1的值趋近于0，大于1的值越来越大），从而导致信息在向前传播的过程中容易出现消失或发散的现象。因此RNN在实际情况中的“记忆能力”是有限的。然而有很多实际问题又需要用到长时记忆，比如阅读一个长篇文章，需要通过前面的叙述去理解后面的叙述。

为了解决RNN梯度消失/爆炸而无法训练的问题，1997年提出了LSTM模型（这个模型很早就提出了，只是一开始没有被大量应用），LSTM（Long Short Term Memory Network）叫做长短期记忆模型。随后又提出了门控循环单元（Gated Recurrent Unit, GRU），它是LSTM的变体，比LSTM模型简单一些。有篇文章对这两个模型的效果进行了比较全面的对比，结论是，LSTM基本在所有问题中都比GRU效果要略好一些，GRU训练过程中的收敛速度要比LSTM快。

普通的RNN中的隐藏状态s只对短期输入敏感，LSTM在原模型的基础上增加了长期记忆层，两者配合形成了长短期记忆。LSTM引入了三个门控单元：输入门，遗忘门和输出门，具体的功能就如名字所表达的含义一样，决定t时刻输入的信息是否被记忆或遗忘。设计的细节如下图所示，

![fig2]({{ site.baseurl}}/images/深度学习笔记-RNN2.png)

图2 RNN，LSTM和GRU隐藏状态的结构示意图。

LSTM是目前使用较为广泛的循环神经网络结构。在语音识别、机器翻译、图片描述等领域都有不少成功应用。
