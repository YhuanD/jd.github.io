---
layout: post
title: 机器学习常用模型速写（四）人工神经网络
date: 2020-06-24 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

这节说说人工神经网络（Artificial Neural Network, ANN）模型。这一模型很有仿生学的思想，其是模仿生物神经系统原理而构造的。比如对于人的大脑，每时每刻都有无数的电流通过，学名叫神经脉冲吧，然后通过每个神经元 (neuron)上的轴突和树突，将脉冲从一个神经元传到另一个神经元。轴突和树突间的连接点叫神经健，当脉冲反复通过某个神经键的时候，就会加强这个键的强度，这就是人学习的过程。比如我们学习任何事情都少不了重复的过程，这个过程的一个作用就是加强相关神经元之间的连结强度，也就是增加这些神经键的强度，或者更准确的说不是加强而是不断的调节其强度。又比如我们说人的思维常常有惯性，有的人思想固执，一根筋之类的，这和这个人长期的重复某些思路或外界长期刺激局部区域的神经元有关，这样的重复使得这条路上的神经元连结的强度远大于其他神经元间的强度。粗略的说强度越强相关的思维过程越顺畅。以上是些科普的知识，在此不考究细节。通过以上的叙述我们可以想像，大量的神经元组成的一个神经网络，各个神经元之间连结强度不同。模型中我们把这个强度当作权重。

一个最简单的ANN模型叫做感知器(perceptron)。下图所示是一个简单的例子，图a所示，有三个特征量x1,x2,x3，输出的分类y为二元分类（取值为-1和1，或者简单的正负）。图b为感知器模型的结构，这个结构看起来比较乱，其实主要就是想构造一个y关于x1,x2,x3的线性函数，这个例子中可以写成y(x1,x2,x3) = sign(w1*x1+w2*x2+w3*x3 + t)，sign表示只取里面式子的符号，w1,w2,w3是三个参数，也就是前面提到的那个权重因子，t是一个偏置因子。这个例子中我们只用线性函数模型，所以上式中是把三个特征量x1,x2,x3加权之后求和。现在我们就可以建立一个算法，通过将数据集中各个点‘导入’模型来调节权重因子，就像‘导入’神经脉冲来调节神经元间的连结强度一样。

![fig1]({{ site.baseurl}}/images/机器学习速写-人工神经网络1.jpg)

图1： 感知器模型举例。

具体的算法这里不详细写出，其是计算方法里经常用的迭代的方法，把w(k+1) 用含w(k)的式子表示出来，并加上一个调整项，这个调整项与(y_m - y_r)成正比，y_m是第k次模型得到的y值，y_r是实际值，这两个差值相当于一个误差项。

以上是简单线性感知器模型，只能用于数据集线性可分的情况，如下图所示。

![fig2]({{ site.baseurl}}/images/机器学习速写-人工神经网络2.jpg)

图2：适用于图1中数据集的分类结果。（线性可分的数据集）

如果是下图这个情况，就不能用一条直线来划分。

![fig3]({{ site.baseurl}}/images/机器学习速写-人工神经网络3.jpg)

图3：不能简单线性划分的例子。

但是实际上在这个例子中我们可以用两条直线来划分，如下图a所示。在这个例子中，我们只需构造两条直线，也就是我们可以简单的汇总两个感知器的结果。模型的结构如b图所示，是个双层的神经网络。

![fig4]({{ site.baseurl}}/images/机器学习速写-人工神经网络4.jpg)

图4: 图3的例子用两条线划分的方法。

在第一个例子中y使用的函数是个符号函数sign，只读取符号，实际上可以把它变成各种其他类型的函数，使输出变成非线性的。

我们还可以进一步扩展这个模型，加入更多的层，根据连接方式的不同有两种神经网络，一是前馈型 (feed-forward)，另一个是递归型 (recurrent)。前馈型是每层的结点只和下一层中结点相连，递归型是可以和同一层或上一层相连。

层数增多我们需要改进算法，我们的最终目的简单的说是要通过不断的修正各个点的权值来得到误差最小的一个模型。这类似于拟合实验数据时常用的最小二乘法，我们有一些列数据点，想要找到一条直线，或曲线，使各个点到这条直线的距离的平方和达到最小值。只不过在神经网络模型中，数据点可以是多维的，模型结构也更复杂。在相对简单的情况下，可以找到误差的全局最小值的点，在多数情况下，难以找到全局最优解，这时可以用梯度下降法等贪心算法来解决，这类贪心算法的问题就是容易陷入局部最小值。梯度下降法要算一个梯度，需要知道各个中间层上隐藏结点的输出值，这些值用一种叫做反向传播 (back-propagation)的方法来进行估计，其过程分为前向阶段和后向阶段，前向阶段先计算第k层结点的输出，再计算第k+1层结点的输出，后向阶段先更新第k+1层的权值，再更新第k层的权值。用第k+1层结点的误差估计第k层结点的误差。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）

欢迎关注我的公众号获取更多内容
![fig]({{ site.baseurl}}/images/wechat_from0_data.jpg)
