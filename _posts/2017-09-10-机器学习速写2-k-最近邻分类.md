---
layout: post
title: 机器学习常用模型速写（二）k-最近邻分类
date: 2020-06-24 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

首先需要介绍机器学习中涉及的三种数据集合，分别叫做训练集（training set），测试集（test set）和验证集（validation set）：

Wiki：

Training set: A set of examples used for learning, that is to fit the parameters of the classifier.

Test set: A set of examples used only to assess the performance of a fully-specified classifier.

Validation set: A set of examples used to tune the hyperparameters of a classifier, for example to choose the number of hidden units in a neural network.

大意：训练集用来训练模型（这里指分类器模型），确定参数；测试集用来评估模型的效果（识别率等各种性能指标）；验证集用来做模型优化和最终确定模型。

比如上一节脊椎动物分类的例子（见下图），我们有很多已知动物的属性和分类标签，每一个动物（每一行）就相当于一个数据点，这些数据点就组成了一个数据集合，一般在实际应用中，通常把这个数据集分成训练集（e.g. 70%）和测试集两部分 （e.g. 30%），也就是用大部分数据作为输入数据用来训练模型，剩下的一少部分用来对模型进行评估。

![fig1]({{ site.baseurl}}/images/机器学习速写-k最近邻1.jpg)

图1：脊椎动物数据集。

k-最近邻算法（k-nearest neighbors algorithm）简称k-NN算法，与上一节介绍的决策树分类算法同样属于有监督学习的分类算法，也就是在给定的数据集中，每个数据点都已知分类标签。

决策树试图建立从属性到分类结果的一个映射模型，最终得出一个确定的‘分类法则’（树），这被叫做积极学习方法（eager learner），与决策树分类算法不同的是，k-最近邻算法并不试图建立确定的模型，它把每个数据点看作是属于d-维空间的一个点，d即是属性的个数（上图中第一栏或column为动物名称，最后一栏为分类标签，中间7栏为属性，所以d=7），然后记录训练集中所有的数据点，再通过查看测试集中的每个点周围‘距离’其最近的几个训练集中的点，来确定测试集中的这个点的类别标签，其中k表示选取的最近邻点的个数。如下图所示的例子中，类别标签共有正负两类，✖️表示的是测试集中待确定类别的一个点，图中分别展示了k取1，2，3三种情况，k=1时，最近的一个点为负类，k=2时，最近邻的两个点为一正一负，k=3时，有两个正一个负。

![fig2]({{ site.baseurl}}/images/机器学习速写-k最近邻2.jpg)

图2：二元分类问题k-最近邻分类算法图示（k=1,2,3）

那么我们怎样决定测试点属于正类还是负类呢？最简单的一种方法是多数表决，看这最近邻的k个点中多数点属于哪个类，就把这个测试点归为哪一类，如果有两个类的数量相等，就随机选择一类。这种方法有个缺点就是没有考虑这k个点距离待测点的距离远近的差别，如下图所示的一个特例，如果用多数表决的方法，由于k个点中负类点多与正类点，待测点会被归为负类，但是正类的点明显距离待测点更近。为了避免这种误判的情况，可以对每个数据点进行包含距离信息的加权，降低较远点对表决的影响力。

![fig3]({{ site.baseurl}}/images/机器学习速写-k最近邻3.jpg)

图3：多数表决方法缺点的特例图示

另外需要提到的一点是关于‘距离’的概念，前文中‘距离’二字加了引号，表示其含义与我们通常理解的不太一样，我们通常用到的d-维空间两点之间的距离是欧氏距离或称为几何距离，但是在这里的‘距离’指的其实是一种邻近性度量，叫做邻近度（proximity），用来度量两点间的相似度或相异度，在上文中，‘距离’越近的点越相似，所以更有可能是同一类。邻近度有多种不同的定义，欧氏距离是其中的一种，另一个直观一点的是余弦角相似度，求的是两个点（两个d维向量）的夹角的余弦值。

k-最近邻算法的一个明显缺点是计算量大，需要计算测试集中的点到训练集中每个点的邻近度来确定k个最近邻的点，通过加入索引可以提高计算效率。另外一个缺点是k-近邻是基于局部信息进行预测的（k个最近邻的点）所以对噪声非常敏感。k-最近邻算法的优点是不需要建立模型，因此在新情境下不必对基于原有数据的模型进行维护，它是一种基于实例的学习方法，是通过计算各个点之间的相似性来确定分类的。另外一个优点是相对于决策树的直线决策边界相比，k-最近邻分类的决策边界灵活可变，通过调整k值来达到最优的分类效果。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）

欢迎关注我的公众号获取更多内容

![fig]({{ site.baseurl}}/images/wechat_from0_data.jpg)





