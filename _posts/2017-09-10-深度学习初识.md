---
layout: post
title: 深度学习初识
date: 2020-06-24 12:25:06 -0700
tags: 
<!-- comments: true -->
categories: machine learning
---

几个月前我报名了udacity的深度学习课程，开始系统的学习深度学习。报名之前我尝试了通过知乎自学，由于自身对于深度学习了解的匮乏导致脑子里缺少系统框架，学起来效率很低。这个课程的内容都是专业人员总结的精华，思路框架很清晰，比脑子空白的自学相对轻松很多。

这个课一开始有个简短的对深度学习的介绍，接着就开始无穷无尽。。的公式推导，第一章中包含大量需要应用公式的编程练习题，结尾还有个需要完成的项目，就是手动搭建神经网络。这道开胃大菜还是有点挑战的，几个公式翻来覆去折腾到恶心的练习题也就算了，最后的项目一度卡死在矩阵运算的行列不匹配问题上，头发因此抓掉了好多==。最近疫情的缘故每天死宅在家，课程学到了rnn，attention，感觉又有点消化不良，进展缓慢。废话不多说，这篇文章打算把深度学习的知识框架理一理，自我感觉知识框架还是非常重要的，框架有了，细节就更容易通过互联网上各类的信息进行填充，在此基础上进行自学也会有的放矢。

无论是机器学习还是深度学习主要解决两类基本问题：分类（classification）和回归 （regression）。之前的几篇文章中，介绍了几种常见的机器学习分类模型，深度学习是从机器学习的一个分支，即神经网络发展起来的，之所以叫深度学习，就在于其结构采用的是多层神经网络，具有一定的“深度”。深度学习在解决很多复杂问题上取得了不错的效果，深度学习爆红应该是从2015年左右开始吧，除了一些相对古老的模型，如卷积神经网络（CNN），循环神经网络（RNN），还有各种变种，如修改了RNN的记忆模块的长短期记忆网络（LSTM，Long short-term memory），后来又在RNN上加注意力机制（attention），后来干脆把RNN都弄没了只保留了注意力机制（这篇很有名的文章叫Attention is all you need, 2017），如此这般不断进行着模型改造。

深度学习相比机器学习有一个很大的特点是，特征无需预先提供，深度学习能够自动从原始数据中提取隐藏的特征。这种特点对于解决复杂问题比如图像识别和自然语言处理，就很有优势了。传统机器学习的一个很重要工作就是特征工程，指的就是将影响结果的因素以标签的形式整理出来，作为模型的输入，比如之前的文章中提到的脊椎动物分类问题，特征标签包括体温、是否水生、是否胎生、是否卵生等等，将这些打好标签的数据输入到模型中进行计算。可以想像如果是个图像分类问题，比如狗品种分类，如果要人工从图像中提取特征作为输入，简直难以想象；或者训练机器去理解自然语言，要人工从语言特点中整理出数字化的标签，既是件费时费力的工作又因为语言的灵活和复杂性而不可能有很好的效果。

深度神经网络（结构如下图所示）训练的基本思路和简单神经网络的思路一样，就是通过调整节点和节点之间的权重（weights）使得计算结果和实际结果越来越接近，首先我们肯定要有个error function（误差函数，loss function应该是更常用的名字，即损失函数，表示的是同一个意思，还有个叫cost function，代价函数，意思基本差不多）来量化这个所谓的“接近”程度，常用的error function如回归问题用的MSE（mean squared error，均方误差），RMSE （root mean squared error，均方根误差），这两个是最小二乘算法里面用过的，还有适用于分类问题的交叉熵（cross entropy loss，这个简单说是概率取对数ln然后求和再加个负号，详细的说就太麻烦了，就先不说了）。还有各种各样你听过没听过，见过没见过的函数可以用来作为error funcion。有了这个函数就有了目标，就是找到这个函数的极小值点，找到这个极小值点我们就可以确定网络中每个节点之间的权重设为多少，这个模型能够最好的预测。

![fig1]({{ site.baseurl}}/images/深度学习初识1.jpg)

图1 普通深度神经网络基本结构。包括输入层、隐藏层、输出层。绿色的圆称为节点，不同层的节点之间用箭头连接，表示两个节点之间存在一个可调节参数w，有多少个箭头就有多少个参数。每一层节点的数量可以自行设定，隐藏层的数量也可调节，输入和输出数据的维度（节点数量）根据具体情况而定。

怎么找到这个函数的极小值呢？遇到求极值的问题我们首先想到的就是求导，导数为0的地方就是函数极值点，对于深度网络这种动不动来个几百万参数的情况，就只能借助于计算机算法，采用梯度下降方法来找极值点。梯度下降算法通常有个形象的解释，就是一个下山的过程（如下图所示），每走一步这个人都停下来算算各个方向的坡度（梯度），然后朝坡度最大的方向往下走一步，每走一步都停下来算算坡度，每次都选择坡度最大的方向，这样一步步就能离山底越来越近。应用到深度网络的训练过程，就是放一份数据进去（一份可以包含多个样本点，一份称为一个batch），然后算出一个error function的值出来，然后通过一个称之为反向传播 （back propogation）的过程，回调节点间的权重，然后不断的循环这个过程，不断的调整权重值。反向传播过程就是个梯度计算的过程，因为深度神经网络包含多个隐藏层，所以梯度值需要在每一层传播，需要用到求偏导的链式法则 （chain rule，就是一层层求偏导，然后每层相乘），这个说起来简单，算起来就复杂了，不过轮子已经有了，就直接带公式就好了。

![fig2]({{ site.baseurl}}/images/深度学习初识2.jpg)

图2 梯度下降算法被形象比喻为一个人下山的过程。这个人距离山底的高度为error function的值，每到一个点都沿着当前点坡度最大的方向继续行走。

这样一看通过反向传播过程，就能最终找到合理的参数了，问题似乎解决了，然而计算梯度有个前提，就是函数必须是连续的，对于分类问题就不满足条件，分类问题是discrete（分离的），而我们需要将其变成continuous（连续的），比如0，1这种二分类问题的函数就是个step function （阶跃函数），这个函数就不能求导，因此使用一些连续函数来代替这个函数（此时又一波函数即将袭来～），常用的有sigmoid function, softmax function, relu function, tanh function等，它们被称为激活函数 (activation function)。其中sigmoid 和softmax函数在二分类问题中是等价的，这两个函数通常还被用来对最后的结果进行约束，将结果变成0，1区间内的概率值。

![fig3]({{ site.baseurl}}/images/深度学习初识3.jpg)

图3 常用激活函数

搭建一个神经网络的流程是怎样的呢？

首先给所有的权重参数赋上一组随机值，包括输入层到隐藏层的一堆权重和隐藏层到输出层的一堆权重，这个倒不是一定要随机赋值的，具体情况中可能有更好的选择。然后给模型喂一个batch的数据，然后依次计算如下项：每个隐藏层的输入值、输出值，最后一层的输入值、输出值，每层的输出值通常都需要用激活函数做用一下。这个过程被称为feedforward（前馈）过程，也就是按照当前的模型进行计算的过程，不涉及参数的调整。接着就是反向传播过程，分别计算最终结果的误差（output error），隐藏层的误差 (hidden error)，然后计算需要调节的权重的大小Δw，包括输入层到隐藏层的和隐藏层到输出层的。前馈和反向传播的过程涉及的运算都是矩阵运算，每个项的计算是有具体公式的。这样一个batch完之后权重参数做一次调整，具体调整多少还可以用learningrate（学习速度）来作为权重加在Δw上来控制速度，保证一次不调整过大或过小。走完整个训练集中的数据称为一个epoch，一轮不够还可以再来多轮epoch，直到loss function足够小，意味着着模型的预测效果足够好。
