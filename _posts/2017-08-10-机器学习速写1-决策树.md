---
layout: post
title: 机器学习常用模型速写（一）决策树
date: 2020-06-24 12:25:06 -0700
tags: 学习笔记
<!-- comments: true -->
categories: machine learning
---

分类和聚类的区别（wikipedia）：

In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.

在机器学习的范畴，分类 (classification)属于有监督学习模型，也就是说，训练集中的数据已经标有已知的类别（目的是通过训练模型将类别未知的数据归入已知的类别）；聚类 (clustering) 属于无监督学习模型，其过程是根据数据内在的相似性将数据划分为不同的类别（类别的个数和性质没有事先标定）。

这篇文章主要讲一下决策树分类模型。

首先，解决分类问题的一般方法可以概括为：用一种学习算法建立一种模型，这个模型能很好的拟合输入数据中属性集和类别标号之间的联系。实际上相当于数学上的映射，将{xi}的属性集合映射到{yi}个类别中。关于训练集，举个栗子，见下图。

![fig1]({{ site.baseurl}}/images/机器学习速写-决策树1.jpg)

图1：脊椎动物训练集。

图示为脊椎动物分类的训练集数据。属性包括：体温 (body temperature)、表皮 (skin cover)、是否胎生 (gives birth)、是否水生 (aquatic creature)等，最后一栏是类别标签，有哺乳类 (mammal)、鸟类 (bird)，鱼类 (fish)、爬行类 (reptile)等。

用某种方法建立好模型后，给定一组类别标签未知的数据，就能够通过模型得到相应的标签。

决策树方法最终会得到一个类似如下图所示的树模型。类似流程图结构。

![fig2]({{ site.baseurl}}/images/机器学习速写-决策树2.jpg)

图2：哺乳动物非哺乳动物分类问题的决策树。

上图中标注了一些概念：根结点 (root node)，叶结点 (leaf node)，内部节点 (inner node)。图中所示的例子是脊椎动物数据集的简化版，最终的类别标签只有两类：哺乳类，非哺乳类。

下图展示了得到树模型之后，将不带类别标签的一组数据导入模型，输出类别标签的过程。

![fig3]({{ site.baseurl}}/images/机器学习速写-决策树3.jpg)

图3：通过得到的树模型判断火烈鸟属于非哺乳类动物的过程。

构建决策树有几种不同的算法，包括 ID3/4/5, CLS, ASSISTANT, 和 CART算法。这里不详述这些具体的算法，只概括的说说一般的方法，另外还有一些和模型测评，优化相关的基本概念。

Hunt算法是很多算法的基础。Hunt算法主要试图将数据集划分成纯度更纯的子集，具体步骤是递归的调用以下算法（直到所有的子集都只含有一个类别）：

（1）如果集合中含有多个类别，则选择一个属性，将集合分裂成较小的子集。

（2）如果一个集合中只含有一个类别，则停止分裂，这个集合变成一个叶结点。

这个简单的算法会有一些缺陷，无法解决一些特殊情况。这里不讲。

属性会有不同的类型，例如二元（是否），标称（多于两个类别）型，连续型。对于连续型属性，需要先将其离散化，可以通过划分范围变成二元，或多路的类型。示例见下图：

![fig4]({{ site.baseurl}}/images/机器学习速写-决策树4.jpg)

图4：连续型属性转化为非连续型属性（举例：年收入）。

一般给定数据都有多个属性，这样就有一个怎样选择划分属性的问题，之前提到我们主要的一个思想就是使划分出来的子集有较高的纯度，这样我们就可以根据划分后状态的纯度的高低来选择最佳划分方法。不纯性度量的例子或者指标包括：熵（entropy）、基尼不纯度（Gini impurity）和分类误差 (classification error)。熵这个词在很多领域会涉及到，其一般用来衡量系统的无序程度，这里熵差代表的是信息增益（information gain）。这些指标也会有些缺陷，由此引入增益率的概念，这里不讲。

在实际问题中处理大量包含噪声和坏点的数据时，使用决策树最终可能得到的树结构过于庞大，这就涉及到树剪枝 （tree running），以防止过分拟合（overfitting）的发生。相对应于过分拟合还要考虑拟合不足的情况。

关于过拟合，通常认为模型的复杂度对过分拟合有影响，理想的复杂度是能产生最低泛化误差的模型的复杂度。泛化能力，指的是模型对未知数据的预测能力。

关于决策树分类先说到这。很多书里把随机森林（random forest）和决策树放到一起讲，在这本书里是放到组合方法一节里讲的，其是通过对样本进行多次抽样，构造多个决策树模型，然后组合这些决策树得到一个最优的模型，这就涉及到抽样、组合等概率、统计方法，由此可见统计学在数据挖掘中的重要作用。

（图片内容来源于数据挖掘经典书籍《数据挖掘导论》（《Introduction to Data Mining》）[美]作者Pang-Ning Tan，Michael Steinbach，Vipin Kumar 合著。）







